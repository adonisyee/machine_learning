{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\n",
    "\n",
    "I noticed that the key frame annotations are not always accurately labeled, for example\n",
    "some have the left/right labels backwards for what the keyframes show, some keyframes don't\n",
    "show movements indicative of what is labele, some keyframes look different from others (different\n",
    "subject or completely different outfits/angles), some keyframes no in portrait mode, etc. \n",
    "\n",
    "I observed that the keypoint annotations are sometimes mislabeled, as for some frames they\n",
    "are not even on the body or not even in the picture frame, some are missing many keypoints, \n",
    "some had left/right keypoints backwards, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\n",
    "\n",
    "There is a wide variety in the lighting, angles of video capture, distance of subject \n",
    "from camera, position of the subject in the frame, clarity of the capture, etc. for each \n",
    "video. It seems it would be very difficult to analyze the data if the actual key frame\n",
    "images were used, and using keypoint annotations there would be some variability in \n",
    "the locations of the points due to the different angles people recorded at and the various\n",
    "poses of people that did the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) \n",
    "\n",
    "I noticed that the keypoint annotations are not that accurate, as some keypoints are \n",
    "not on the subject, some are on the wrong side, and some are in the wrong locations. Also, \n",
    "the bounding boxes often don't enclose the entire subject and sometimes more than one \n",
    "subject is detected even if there is only one.\n",
    "\n",
    "The sampling rate seems like it did adequately capture the movement, as for the squat example\n",
    "you can see frames for which the subject raised their arms, began to squat, went into a full\n",
    "squat, and then came back up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) I saw that compared to the normalized aligned Neck keypoints, the raw keypoints had \n",
    "much more variability and outliers. The raw graph had keypoints scattered across the graph\n",
    "with a very large area of points as the main cluster, but the normalized aligned graph\n",
    "had almost all the points centered in a cluster of small area around 0 with only a few\n",
    "outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Code:\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "    \n",
    "    out = None\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    D = np.prod(x.shape[1:])\n",
    "    xM = x.reshape(N, D)\n",
    "    out = np.dot(xM, w) + b\n",
    " \n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "    \n",
    "def affine_backward(dout, cache):\n",
    "\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    D = np.prod(x.shape[1:])\n",
    "    xM = x.reshape(N, D)\n",
    "    dx = np.dot(dout, w.T).reshape(x.shape)\n",
    "    dw = np.dot(xM.T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "\n",
    "    return dx, dw, db\n",
    "    \n",
    "2. \n",
    "From lecture, the partial derivative for component i in the backward pass \n",
    "is equal to $$\\sum_{j=1}^{k} \\frac{dL}{dz} \\Delta_i z_j $$\n",
    " \n",
    "For dx: \n",
    "the derivative with respect to x of xw + b is w, so the partial\n",
    "derivative for the backward pass is equal to the dot product of\n",
    "the upstream derivative, dout, with w transpose. Then dx should \n",
    "be properly reshaped to be the same shape as x.\n",
    "\n",
    "For dw:\n",
    "the derivative with respect to w of xw + b is x, so the partial\n",
    "derivative for the backward pass is equal to the dot product of \n",
    "x transpose with the upstream derivative, dout. x is first \n",
    "reshaped to be dimension (N, D) where D is d1 * ... * dk to \n",
    "be able to perform the multiplication. \n",
    "\n",
    "For db:\n",
    "the derivative with respect to b of xw + b is 1, so the partial\n",
    "derivative for the backward pass is equal to the dot product of\n",
    "the upstream derivative, dout, with 1, which is equal to the sum\n",
    "over dout.\n",
    "\n",
    "\n",
    "3. Output of numerical gradient checking:\n",
    "\n",
    "grad:\n",
    "\n",
    "[[-1.04345093  0.20915809 -0.47062589 -0.58676516]\n",
    "\n",
    " [-1.58678198  0.45455414  0.13067486 -0.30436602]\n",
    " \n",
    " [-0.95020206  0.30773446  0.29861707 -0.02918212]]\n",
    " \n",
    "ngrad:\n",
    "\n",
    "[[-1.04345093  0.20915809 -0.47062589 -0.58676516]\n",
    "\n",
    " [-1.58678198  0.45455414  0.13067486 -0.30436602]\n",
    " \n",
    " [-0.95020206  0.30773446  0.29861707 -0.02918212]]\n",
    " \n",
    "4. No inline questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Code:\n",
    "\n",
    "def relu_forward(x):\n",
    "\n",
    "    out = None\n",
    "   \n",
    "    out = x * (x>=0)\n",
    "    \n",
    "    cache = x\n",
    "    return out, cache\n",
    "    \n",
    "def relu_backward(dout, cache):\n",
    "\n",
    "    dx, x = None, cache\n",
    "    \n",
    "    dx = dout * (x>=0)\n",
    "    \n",
    "    return dx\n",
    "\n",
    "2. \n",
    "The ReLU forward function replaces x values where x < 0 with 0. The ReLU backward function does the same thing but replaces the derivative values from dout with 0 where x < 0.\n",
    "\n",
    "\n",
    "3. Output of numerical gradient checking:\n",
    "\n",
    "grad:\n",
    "\n",
    "[[ 0.33349295  1.23562329 -0.12733505]\n",
    "\n",
    " [ 0.16847097  0.         -0.        ]]\n",
    " \n",
    " ngrad:\n",
    " \n",
    "[[ 0.33349295  1.23562329 -0.12733505]\n",
    "\n",
    " [ 0.16847097  0.          0.        ]]\n",
    "\n",
    "4. The Sigmoid function has the vanishing gradient problem as well as the tanh function. \n",
    "This happens when the inputs of most training points cause the gradient to become almost \n",
    "zero, caused when sigmoid output is close to 0 or 1 and when tanh output is close to -1 \n",
    "or 1. For these values, the sigmoid and tanh functions are at near flat spots graphically \n",
    "(near 0 gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Code:\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "\n",
    "    loss = 0.0\n",
    "    dx = None\n",
    "    \n",
    "    m = np.max(x, axis=1, keepdims=True)\n",
    "    N = x.shape[0]\n",
    "    val = x - m\n",
    "    exp_val = np.exp(val)\n",
    "    exp_sum = np.sum(exp_val, axis=1, keepdims=True)\n",
    "    log_exp_sum = np.log(exp_sum)\n",
    "    loss = np.mean((-val + log_exp_sum)[np.arange(N), y])\n",
    "    dx = exp_val/exp_sum\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    \n",
    "    return loss, dx\n",
    "\n",
    "2. From lecture, the loss function for cross entropy loss is \n",
    "$-\\sum_{i=1}^{k} y_i ln(z_i)$ where z is softmax activation output, and \n",
    "the gradient of that is $\\frac{-y_j}{z_j}$. So, dx is \n",
    "$$\\frac{-y_j}{z_j} \\frac{dz_j}{dx}$$\n",
    "Also from lecture, the gradient of the softmax output is $z_j(1-z_j)$ if $j=i$, and $-z_j z_i$ if $j\\neq i$\n",
    "So, $$dx = -y_j (1-z_j) - \\sum_{k\\neq j} y_k (-z_k z_i)$$\n",
    "$$= z_j (y_j + \\sum_{k\\neq 1} y_k) - y_j$$\n",
    "which equals $z_j - y_j$ because $y_j + \\sum_{k\\neq 1} y_k = 1$. \n",
    "\n",
    "So, in the code for dx we subtract 1 from the softmax activation outputs where $y=1$.\n",
    "\n",
    "In the code, to get a singular value I took the mean of the losses and divided by N.\n",
    "\n",
    "3. Output of numerical gradient checking:\n",
    "\n",
    "grad:\n",
    "\n",
    "[[ 0.18840093 -0.317676    0.12927507]\n",
    "\n",
    " [ 0.31719863 -0.40597951  0.08878088]]\n",
    " \n",
    "ngrad:\n",
    "\n",
    "[[ 0.18840093 -0.317676    0.12927507]\n",
    "\n",
    " [ 0.31719863 -0.40597951  0.08878088]]\n",
    " \n",
    "4. No inline questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameter combination: \n",
    "\n",
    "hidden_dim: 5000, 'lr_decay': .9, 'num_epochs': 10,\n",
    "'batch_size': 50, 'learning_rate': .0001, weight_scale: .01\n",
    "\n",
    "Best validation accuracy: \n",
    "0.930833\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden_dim values tried: 10, 100, 1000, 5000, 4000\n",
    "\n",
    "'lr_decay' values tried: .8, .7, .9\n",
    "\n",
    "'num_epochs' values tried: 5, 15, 10\n",
    "\n",
    "'batch_size' values tried: 10, 100, 50\n",
    "\n",
    "'learning_rate' values tried: .01, .001, .0001\n",
    "\n",
    "weight_scale: .1, .01, .2, .001\n",
    "\n",
    "In my exploration, I focused on one parameter at a time, increasing with\n",
    "larger and smaller values until I found one that maximized the validation\n",
    "accuracy. I started with the hidden_dim values, increasing until I saw \n",
    "a noticeable increase in accuracy at 1000, then experimented with other\n",
    "values one at a time. After I thought I found the best hyperparameter \n",
    "values I increased the dimensions again until my validation accuracy\n",
    "stopped increasing. I did not do any transformations to the data before\n",
    "testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of best parameter training:\n",
    "\n",
    "(Iteration 1 / 720) loss: 49.434835\n",
    "\n",
    "(Epoch 0 / 10) train acc: 0.109000; val_acc: 0.125000\n",
    "\n",
    "(Epoch 1 / 10) train acc: 0.846000; val_acc: 0.821667\n",
    "\n",
    "(Iteration 101 / 720) loss: 0.865761\n",
    "\n",
    "(Epoch 2 / 10) train acc: 0.957000; val_acc: 0.910833\n",
    "\n",
    "(Iteration 201 / 720) loss: 0.221717\n",
    "\n",
    "(Epoch 3 / 10) train acc: 0.962000; val_acc: 0.924167\n",
    "\n",
    "(Epoch 4 / 10) train acc: 0.967000; val_acc: 0.923333\n",
    "\n",
    "(Iteration 301 / 720) loss: 0.008394\n",
    "\n",
    "(Epoch 5 / 10) train acc: 0.981000; val_acc: 0.924167\n",
    "\n",
    "(Iteration 401 / 720) loss: 0.001829\n",
    "\n",
    "(Epoch 6 / 10) train acc: 0.978000; val_acc: 0.930000\n",
    "\n",
    "(Iteration 501 / 720) loss: 0.024000\n",
    "\n",
    "(Epoch 7 / 10) train acc: 0.990000; val_acc: 0.938333\n",
    "\n",
    "(Epoch 8 / 10) train acc: 0.990000; val_acc: 0.925833\n",
    "\n",
    "(Iteration 601 / 720) loss: 0.022938\n",
    "\n",
    "(Epoch 9 / 10) train acc: 0.992000; val_acc: 0.930833\n",
    "\n",
    "(Iteration 701 / 720) loss: 0.154167\n",
    "\n",
    "(Epoch 10 / 10) train acc: 0.994000; val_acc: 0.930833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer combinations tried:\n",
    "\n",
    "One layer: 4000, because 1 layer was experimented with already in 2.2\n",
    "\n",
    "Two layers: [1000, 500], [1000, 1000], [2000, 1000], [3000, 3000], [2000, 4000]\n",
    "\n",
    "Three layers: [1000, 1000, 1000], [2000, 2000, 2000], [3000, 1000, 3000], [1000, 3000, 1000]\n",
    "\n",
    "Four layers: [500, 1000, 750, 2000], [3000, 1000, 2000, 3000], [1000, 2000, 3000, 4000]\n",
    "\n",
    "Best result: Two hidden layers, [2000, 4000]\n",
    "\n",
    "Validation Accuracy: 0.934167\n",
    "\n",
    "Hyperameters kept the same as in 2.2\n",
    "\n",
    "I tried different combinations for each network, experimenting with different variations\n",
    "of lower/higher dims for each layer, and trying not to use too many dims for 3 or 4 layers\n",
    "because it made it made computation slow on my computer. Accuracies ranged from 75%-92% \n",
    "for all but the best result, with networks that used less that 1000 nodes doing the worst.\n",
    "The best result came from a 2 layer network for me, with other hyperparameters the same as \n",
    "in 2.2. Again no transformations were done to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Validation Accuracies for [2000, 4000] netwrk:\n",
    "\n",
    "(Iteration 1 / 720) loss: 8.675402\n",
    "\n",
    "(Epoch 0 / 10) train acc: 0.120000; val_acc: 0.125000\n",
    "\n",
    "(Epoch 1 / 10) train acc: 0.792000; val_acc: 0.787500\n",
    "\n",
    "(Iteration 101 / 720) loss: 0.217967\n",
    "\n",
    "(Epoch 2 / 10) train acc: 0.919000; val_acc: 0.901667\n",
    "\n",
    "(Iteration 201 / 720) loss: 0.098382\n",
    "\n",
    "(Epoch 3 / 10) train acc: 0.901000; val_acc: 0.895833\n",
    "\n",
    "(Epoch 4 / 10) train acc: 0.946000; val_acc: 0.919167\n",
    "\n",
    "(Iteration 301 / 720) loss: 0.267109\n",
    "\n",
    "(Epoch 5 / 10) train acc: 0.964000; val_acc: 0.926667\n",
    "\n",
    "(Iteration 401 / 720) loss: 0.067065\n",
    "\n",
    "(Epoch 6 / 10) train acc: 0.983000; val_acc: 0.925833\n",
    "\n",
    "(Iteration 501 / 720) loss: 0.126912\n",
    "\n",
    "(Epoch 7 / 10) train acc: 0.971000; val_acc: 0.930000\n",
    "\n",
    "(Epoch 8 / 10) train acc: 0.973000; val_acc: 0.933333\n",
    "\n",
    "(Iteration 601 / 720) loss: 0.069198\n",
    "\n",
    "(Epoch 9 / 10) train acc: 0.975000; val_acc: 0.931667\n",
    "\n",
    "(Iteration 701 / 720) loss: 0.212222\n",
    "\n",
    "(Epoch 10 / 10) train acc: 0.980000; val_acc: 0.934167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) For my first model attempt, I started with \n",
    "\n",
    "parameters: batch_size=128, learning_rate=.001, num_epochs=5 padding=2\n",
    "\n",
    "architecture: \n",
    "\n",
    "NeuralNet(\n",
    "\n",
    "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
    "  \n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  \n",
    "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
    "  \n",
    "  (fc1): Linear(in_features=95040, out_features=120, bias=True)\n",
    "  \n",
    "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
    "  \n",
    "  (fc3): Linear(in_features=84, out_features=8, bias=True))\n",
    "  \n",
    "validation accuracy: 52.0 %\n",
    "\n",
    "I decided to then try increasing the number of convolution layers, with new layers having \n",
    "smaller kernel size, and increased the output layers as much as my gpu would allow without \n",
    "crashing. Increasing output layers required me to lower my batch size also for my code to run.\n",
    "I also increased the number of epochs to make sure there was still room to learn before \n",
    "overfitting. In implementing my convolutional layers I drew inspiration from AlexNet when\n",
    "I was choosing dimensions, as piazza mentioned that nets similar to that would be unlikely\n",
    "to perform poorly, but used a stride of 1 for all my convolution layers and never used a kernel\n",
    "size larger than 5. I also added regularization because I thought my low validation \n",
    "accuracies were a product of overfitting too quickly.\n",
    "\n",
    "architecture:\n",
    "\n",
    "NeuralNet(\n",
    "\n",
    "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
    "  \n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  \n",
    "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1))\n",
    "  \n",
    "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1))\n",
    "  \n",
    "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
    "  \n",
    "  (fc1): Linear(in_features=79872, out_features=200, bias=True)\n",
    "  \n",
    "  (fc2): Linear(in_features=200, out_features=100, bias=True)\n",
    "  \n",
    "  (fc3): Linear(in_features=100, out_features=8, bias=True))\n",
    "  \n",
    "validation accuracy: 62.56410256410256 %\n",
    "\n",
    "After restarting my computer Google Colab suddenly allowed me to use much larger numbers of layers\n",
    "without crashing, so I added a convolutional layer and increased the output layer sizes to 4000 to \n",
    "be similar in size to my multilayered net. I also decreased my learning rate by a factor of 10, \n",
    "calculated the means and standard deviations for each color channel, removed regularization \n",
    "because it was now preventing my model from learning as quick as I wanted, and reduced the epochs \n",
    "back to 5 because I thought the model was overfitting.\n",
    "\n",
    "architecture:\n",
    "\n",
    "NeuralNet(\n",
    "\n",
    "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
    "  \n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  \n",
    "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1))\n",
    "  \n",
    "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1))\n",
    "  \n",
    "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
    "  \n",
    "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
    "  \n",
    "  (fc1): Linear(in_features=15360, out_features=4000, bias=True)\n",
    "  \n",
    "  (fc2): Linear(in_features=4000, out_features=4000, bias=True)\n",
    "  \n",
    "  (fc3): Linear(in_features=4000, out_features=8, bias=True))\n",
    "  \n",
    "validation accuracy: 77.43589743589743 %\n",
    "\n",
    "For my final model, I decided to run the previous one for 20 epochs even though training\n",
    "loss reached near zero because validation accuracy continued to increase, although very slowly. \n",
    "\n",
    "validation accuracy: 78.05128205128206 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) As you can see from the table, you would need more than 2 million additional units \n",
    "in order to implement an FCC network comparable to the convolutional layers of the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) I ran the model on random frames for one epoch because training was so slow that \n",
    "just one epoch plus calculating validation loss took almost 4 hours. The training \n",
    "loss was .2213 and validation accuracy was 66.10555847799404 %. \n",
    "\n",
    "reach: 0.340956340956341\n",
    "\n",
    "squat: 0.4829749103942652\n",
    "\n",
    "inline: 0.5451327433628319\n",
    "\n",
    "lunge: 0.8029993183367417\n",
    "\n",
    "hamstrings: 0.787920384351407\n",
    "\n",
    "stretch: 0.8116683725690891\n",
    "\n",
    "deadbug: 0.6883468834688347\n",
    "\n",
    "pushup: 0.7184873949579832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key frame</th>\n",
       "      <th>random frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reach</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.340956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>squat</th>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.482975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inline</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.545133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lunge</th>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.802999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamstrings</th>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.787920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stretch</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.811668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deadbug</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.688347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pushup</th>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.718487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall %</th>\n",
       "      <td>78.051282</td>\n",
       "      <td>66.105558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            key frame  random frame\n",
       "reach        0.720000      0.340956\n",
       "squat        0.813333      0.482975\n",
       "inline       0.800000      0.545133\n",
       "lunge        0.773333      0.802999\n",
       "hamstrings   0.753333      0.787920\n",
       "stretch      0.833333      0.811668\n",
       "deadbug      0.733333      0.688347\n",
       "pushup       0.826667      0.718487\n",
       "overall %   78.051282     66.105558"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_map = ['reach','squat','inline','lunge','hamstrings','stretch',\n",
    "             'deadbug','pushup', 'overall %']\n",
    "random_frame = [0.340956340956341, 0.4829749103942652, \n",
    "                0.5451327433628319, 0.8029993183367417, \n",
    "                0.787920384351407, 0.8116683725690891,\n",
    "                0.6883468834688347, 0.7184873949579832,\n",
    "                66.10555847799404]\n",
    "key_frame = [0.72, 0.8133333333333334,\n",
    "             0.8, 0.7733333333333333,\n",
    "             0.7533333333333333, 0.8333333333333334,\n",
    "             0.7333333333333333, 0.8266666666666667,\n",
    "             78.05128205128206]\n",
    "\n",
    "table = pd.DataFrame(index = label_map)\n",
    "table['key frame'] = key_frame\n",
    "table['random frame'] = random_frame\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) I observed that my better model was the key frame one because  was able to \n",
    "train it more because it did not take so many hours to perform. I chose to \n",
    "explore the errors for my model for key frames. I also noticed that reach was \n",
    "sometimes classified as inline while hamstring was sometimes classified as deadbug.\n",
    "most pictures that were misclassified were ones in which images were a little \n",
    "blurrier, cut off, or the subject was in a position that was somewhat\n",
    "different from the normal keyframe position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) My kaggle username is ayee and my best submission was:\n",
    "\n",
    "0.69979\n",
    "\n",
    "I think my model would have been able to do better but Google Colab\n",
    "went through my data so slowly for random frames that training was so \n",
    "slow I was not able to experiment with parameters much, and was only \n",
    "able to get it to train for four epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
